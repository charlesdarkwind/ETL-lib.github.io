\hypertarget{namespace_e_t_l_1_1trusted__tables}{}\doxysection{E\+T\+L.\+trusted\+\_\+tables Namespace Reference}
\label{namespace_e_t_l_1_1trusted__tables}\index{ETL.trusted\_tables@{ETL.trusted\_tables}}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \mbox{\hyperlink{namespace_e_t_l_1_1trusted__tables_a1b375def0b26a8bff402b8b5b6bb2667}{read}} (config, table, $\ast$$\ast$dataframe\+\_\+reader\+\_\+options)
\item 
def \mbox{\hyperlink{namespace_e_t_l_1_1trusted__tables_adf0c76d85e3176d6fe54bf0a1294afc2}{run\+\_\+transformation}} (transformation, df, $\ast$args, $\ast$$\ast$kwargs)
\item 
def \mbox{\hyperlink{namespace_e_t_l_1_1trusted__tables_acd949af23efc5f1bff2ec048d3149d94}{write}} (config, table, num\+\_\+files=10, df=None, transformation=None, incremental=False, $\ast$$\ast$dataframe\+\_\+writer\+\_\+options)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}This module provides utility functions to select/read/write JSON data from the trusted zone.

The names and structure of tables, folders and paths must conform to the library's standard convention:

  - trusted-zone -| - data_source - ...
              |
              |
              | - data_source - ...
              |
              |
              | - yammer -  | - table_name
                            | - table_name
                            | - Messages
                            | - Users
                            | - Likes
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\mbox{\Hypertarget{namespace_e_t_l_1_1trusted__tables_a1b375def0b26a8bff402b8b5b6bb2667}\label{namespace_e_t_l_1_1trusted__tables_a1b375def0b26a8bff402b8b5b6bb2667}} 
\index{ETL.trusted\_tables@{ETL.trusted\_tables}!read@{read}}
\index{read@{read}!ETL.trusted\_tables@{ETL.trusted\_tables}}
\doxysubsubsection{\texorpdfstring{read()}{read()}}
{\footnotesize\ttfamily def E\+T\+L.\+trusted\+\_\+tables.\+read (\begin{DoxyParamCaption}\item[{}]{config,  }\item[{}]{table,  }\item[{$\ast$$\ast$}]{dataframe\+\_\+reader\+\_\+options }\end{DoxyParamCaption})}

\begin{DoxyVerb}Read trusted zone's json files.

Parameters
----------
config: Config
  Config instance

table: str

dataframe_reader_options
\end{DoxyVerb}
 \mbox{\Hypertarget{namespace_e_t_l_1_1trusted__tables_adf0c76d85e3176d6fe54bf0a1294afc2}\label{namespace_e_t_l_1_1trusted__tables_adf0c76d85e3176d6fe54bf0a1294afc2}} 
\index{ETL.trusted\_tables@{ETL.trusted\_tables}!run\_transformation@{run\_transformation}}
\index{run\_transformation@{run\_transformation}!ETL.trusted\_tables@{ETL.trusted\_tables}}
\doxysubsubsection{\texorpdfstring{run\_transformation()}{run\_transformation()}}
{\footnotesize\ttfamily def E\+T\+L.\+trusted\+\_\+tables.\+run\+\_\+transformation (\begin{DoxyParamCaption}\item[{}]{transformation,  }\item[{}]{df,  }\item[{$\ast$}]{args,  }\item[{$\ast$$\ast$}]{kwargs }\end{DoxyParamCaption})}

\begin{DoxyVerb}Run transformation on df using arguments.

Pass a tuple like (transform, df, myarg, other_arg=val)

Parameters
----------
transformation: (pyspark.sql.DataFrame, args, kwargs) -> pyspark.sql.DataFrame
  A function taking a df and returning a df.

df: pyspark.sql.DataFrame

Returns
-------
pyspark.sql.DataFrame
\end{DoxyVerb}
 \mbox{\Hypertarget{namespace_e_t_l_1_1trusted__tables_acd949af23efc5f1bff2ec048d3149d94}\label{namespace_e_t_l_1_1trusted__tables_acd949af23efc5f1bff2ec048d3149d94}} 
\index{ETL.trusted\_tables@{ETL.trusted\_tables}!write@{write}}
\index{write@{write}!ETL.trusted\_tables@{ETL.trusted\_tables}}
\doxysubsubsection{\texorpdfstring{write()}{write()}}
{\footnotesize\ttfamily def E\+T\+L.\+trusted\+\_\+tables.\+write (\begin{DoxyParamCaption}\item[{}]{config,  }\item[{}]{table,  }\item[{}]{num\+\_\+files = {\ttfamily 10},  }\item[{}]{df = {\ttfamily None},  }\item[{}]{transformation = {\ttfamily None},  }\item[{}]{incremental = {\ttfamily False},  }\item[{$\ast$$\ast$}]{dataframe\+\_\+writer\+\_\+options }\end{DoxyParamCaption})}

\begin{DoxyVerb}Writes the spark dataframe in json format to the trusted zone.

Parameters
----------
config: Config
  Config instance

table: str

num_files: int
  How many files to write

df: pyspark.sql.DataFrame
  Dataframe to write. Leave None to read table from curated.

transformation: (pyspark.sql.DataFrame) -> pyspark.sql.DataFrame, optional
  A function taking a df and returning a df.

incremental: bool, optional
  To perform incremental load based on the curated-zone's control table content.
  Only possible when not passing your own DF.

dataframe_writer_options
\end{DoxyVerb}
 